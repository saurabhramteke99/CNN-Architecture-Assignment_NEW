{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CNN Architecture | Assignment"
      ],
      "metadata": {
        "id": "_7tnfv0I6Fa8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1: What is the role of filters and feature maps in Convolutional Neural Network (CNN)?"
      ],
      "metadata": {
        "id": "dPSV0d0E6bJi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0997478c"
      },
      "source": [
        "Filters and feature maps are fundamental components of Convolutional Neural Networks (CNNs). Here's a breakdown of their roles:\n",
        "\n",
        "*   **Filters (or Kernels):** These are small matrices that slide over the input image (or a previous layer's feature map). Each filter is designed to detect a specific pattern or feature in the image, such as edges, corners, or textures. During the convolution operation, the filter is multiplied element-wise with the corresponding portion of the input, and the results are summed to produce a single value in the output feature map. Different filters in a layer learn to detect different features.\n",
        "\n",
        "*   **Feature Maps:** The output of the convolution operation is a feature map. Each feature map represents the result of applying a specific filter to the entire input. It shows where in the input the feature detected by that filter is present and how strongly it is present. For example, if a filter is designed to detect vertical edges, its corresponding feature map will highlight the locations in the image where vertical edges are found. As the data progresses through the layers of the CNN, the feature maps become more abstract, representing higher-level features like object parts or even complete objects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10bd2f86"
      },
      "source": [
        "## Question 2: Explain the concepts of padding and stride in CNNs(Convolutional Neural Network). How do they affect the output dimensions of feature maps?\n",
        "\n",
        "Ans : Padding and stride are two important concepts in Convolutional Neural Networks (CNNs) that influence the output dimensions of feature maps and the overall behavior of the network.\n",
        "\n",
        "*   **Padding:** Padding involves adding extra pixels (typically with values of zero) around the border of the input image or feature map. The primary purpose of padding is to prevent the loss of information at the edges of the input during convolution. Without padding, the pixels at the edges are only included in a few convolution operations, while the pixels in the center are included in many. This can lead to the underrepresentation of edge features in the output feature map. Padding ensures that edge pixels are processed more equally, helping to preserve spatial information. Padding can also be used to maintain the spatial dimensions of the output feature map the same as the input. Common types of padding include \"valid\" (no padding) and \"same\" (padding to maintain output dimensions).\n",
        "\n",
        "*   **Stride:** Stride refers to the number of pixels the filter shifts over the input during each convolution step. A stride of 1 means the filter moves one pixel at a time. A stride of 2 means the filter moves two pixels at a time, effectively downsampling the spatial dimensions of the output feature map. Increasing the stride reduces the size of the output feature map, which can help to reduce computational cost and the number of parameters in the network.\n",
        "\n",
        "**How they affect output dimensions:**\n",
        "\n",
        "The output dimensions of a feature map after a convolutional layer are determined by the input dimensions, the filter size, the padding, and the stride. The formula for calculating the output dimension (either height or width) is:\n",
        "\n",
        "$$ Output Dimension = \\lfloor \\frac{(Input Dimension - Filter Size + 2 \\times Padding)}{Stride} \\rfloor + 1 $$\n",
        "\n",
        "Where:\n",
        "*   Input Dimension: The spatial dimension (height or width) of the input.\n",
        "*   Filter Size: The spatial dimension (height or width) of the filter.\n",
        "*   Padding: The amount of padding applied to one side of the input. If padding is applied symmetrically, the total padding is $2 \\times Padding$.\n",
        "*   Stride: The number of pixels the filter shifts.\n",
        "\n",
        "*   **Padding:** Increases the effective input dimension, which can lead to a larger output feature map. With \"same\" padding, the output dimensions can be the same as the input dimensions.\n",
        "*   **Stride:** Reduces the number of convolution operations, leading to a smaller output feature map. A larger stride results in a greater reduction in dimensions.\n",
        "\n",
        "By carefully selecting padding and stride values, CNN architects can control the spatial dimensions of feature maps at each layer, influencing the network's ability to capture features at different scales and manage computational resources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5c18c2f"
      },
      "source": [
        "## Question 3: Define receptive field in the context of CNNs. Why is it important for deep architectures?\n",
        "\n",
        "Ans: In the context of Convolutional Neural Networks (CNNs), the **receptive field** of a neuron in a particular layer is the region in the original input image that influences that neuron's activation. In simpler terms, it's the area of the input image that a specific filter \"sees\" when computing the output value at a particular location in a feature map.\n",
        "\n",
        "**How it's determined:**\n",
        "\n",
        "The receptive field size is not just the size of the filter in the current layer. It's a cumulative effect of the filter sizes and strides in all the preceding layers. As you go deeper into the network, the receptive field of neurons in later layers becomes larger, covering a wider area of the original input image.\n",
        "\n",
        "**Why it's important for deep architectures:**\n",
        "\n",
        "The increasing receptive field size in deeper layers is crucial for the effectiveness of CNNs in deep architectures:\n",
        "\n",
        "*   **Hierarchical Feature Extraction:** Early layers with smaller receptive fields focus on extracting local, low-level features like edges and corners. As the receptive field grows in deeper layers, neurons can combine these low-level features to detect more complex, high-level features like textures, shapes, and eventually, object parts or even entire objects. This hierarchical extraction of features is a key reason why CNNs are so powerful for image recognition tasks.\n",
        "*   **Contextual Information:** A larger receptive field allows neurons to incorporate more contextual information from the input image when making a decision. For example, to identify an object like a car, a neuron in a deep layer needs to consider not just a small patch of pixels, but a larger area that includes wheels, windows, and the overall shape. The receptive field enables this by providing a wider view of the input.\n",
        "*   **Robustness to Translations:** The increasing receptive field also contributes to the network's robustness to translations (shifting the object's position in the image). Because deeper layers consider larger areas, a slight shift in the object's position is less likely to significantly change the activation patterns in these layers.\n",
        "*   **Efficiency:** While a larger receptive field could theoretically be achieved with a single large filter, using multiple layers with smaller filters is more efficient in terms of the number of parameters and computational cost. This is because the parameters are shared across different locations in the input through the convolution operation.\n",
        "\n",
        "In summary, the receptive field is a fundamental concept that explains how CNNs build up a hierarchical representation of the input image. Its increasing size in deeper layers allows the network to capture increasingly complex features and contextual information, which is essential for achieving high performance in various computer vision tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5263ed0"
      },
      "source": [
        "## Question 4: Discuss how filter size and stride influence the number of parameters in a CNN.\n",
        "\n",
        "Ans : In a Convolutional Neural Network (CNN), the number of parameters is a critical factor that affects the model's complexity, memory usage, and computational cost. Filter size and stride significantly influence the number of parameters in a convolutional layer.\n",
        "\n",
        "**Filter Size:**\n",
        "\n",
        "*   The number of parameters in a single filter is equal to its spatial dimensions (height * width) multiplied by the number of input channels.\n",
        "*   A convolutional layer typically has multiple filters (also known as kernels). The total number of parameters in a convolutional layer is the number of filters multiplied by the number of parameters in each filter.\n",
        "*   Therefore, increasing the filter size directly increases the number of parameters in a convolutional layer. For example, a 5x5 filter has 25 parameters (assuming a single input channel), while a 3x3 filter has only 9 parameters. Using larger filters in early layers can lead to a rapid increase in the total number of parameters in the network.\n",
        "\n",
        "**Stride:**\n",
        "\n",
        "*   Stride does **not** directly affect the number of parameters within a convolutional layer. The number of parameters in a filter and the number of filters remain the same regardless of the stride value.\n",
        "*   However, stride indirectly influences the number of parameters in subsequent layers. A larger stride reduces the spatial dimensions of the output feature map from the current layer. This smaller output feature map becomes the input to the next layer.\n",
        "*   Since the number of parameters in a convolutional layer depends on the size of the input feature map (specifically, the number of input channels), a smaller input feature map from a layer with a larger stride will result in fewer parameters in the subsequent convolutional layer.\n",
        "\n",
        "**In summary:**\n",
        "\n",
        "*   **Filter size** has a **direct** impact on the number of parameters in a convolutional layer. Larger filters mean more parameters.\n",
        "*   **Stride** has an **indirect** impact on the number of parameters in **subsequent** convolutional layers by reducing the spatial dimensions of the input to those layers. Larger strides lead to smaller input feature maps for the next layer, resulting in fewer parameters in that layer.\n",
        "\n",
        "Therefore, careful consideration of both filter size and stride is essential when designing a CNN architecture to balance the model's capacity to learn complex features with computational efficiency and memory constraints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3811082"
      },
      "source": [
        "## Question 5: Compare and contrast different CNN-based architectures like LeNet, AlexNet, and VGG in terms of depth, filter sizes, and performance.\n",
        "\n",
        "Ans: Here's a comparison of some prominent CNN architectures: LeNet, AlexNet, and VGG, highlighting their characteristics in terms of depth, filter sizes, and performance:\n",
        "\n",
        "| Feature          | LeNet                                     | AlexNet                                       | VGG (e.g., VGG16/VGG19)                           |\n",
        "| :--------------- | :---------------------------------------- | :-------------------------------------------- | :------------------------------------------------ |\n",
        "| **Depth**        | Shallow (a few convolutional layers)      | Deeper than LeNet                             | Much deeper than LeNet and AlexNet                |\n",
        "| **Filter Sizes** | Relatively larger filters (e.g., 5x5)     | Mixed filter sizes (e.g., 11x11, 5x5, 3x3)      | Primarily uses small filters (3x3) throughout     |\n",
        "| **Architecture** | Simple, with convolution and pooling layers | Introduced ReLU activation, dropout, and LRN    | Relies on stacking small filters, pooling layers |\n",
        "| **Parameters**   | Relatively few                          | More parameters than LeNet                    | Significantly more parameters due to depth         |\n",
        "| **Performance**  | Early success on handwritten digits       | Broke records on ImageNet, significant improvement | Achieved state-of-the-art results on ImageNet     |\n",
        "| **Key Idea**     | Basic CNN principles                      | Leveraging GPUs, regularization techniques    | Depth is key, using small, stacked filters        |\n",
        "\n",
        "**Comparison and Contrast:**\n",
        "\n",
        "*   **Depth:** The most significant difference is depth. LeNet is shallow, AlexNet is deeper, and VGG is much deeper. This increase in depth allows later architectures to learn more complex and hierarchical feature representations.\n",
        "*   **Filter Sizes:** LeNet used larger filters, while AlexNet used a mix. VGG's key contribution was demonstrating that using only small (3x3) filters throughout the network, stacked in multiple layers, could be highly effective. This is because stacking two 3x3 filters with a stride of 1 has an effective receptive field equivalent to a 5x5 filter, but with fewer parameters.\n",
        "*   **Performance:** Each architecture represented a significant step forward in performance on image recognition tasks compared to its predecessors. AlexNet's success on ImageNet in 2012 was a turning point, and VGG further improved upon this by exploring the impact of depth.\n",
        "*   **Computational Cost:** With increasing depth and parameters, the computational cost also increased significantly from LeNet to VGG.\n",
        "\n",
        "In essence, the evolution from LeNet to VGG shows a trend towards deeper networks, the use of smaller, stacked filters, and the incorporation of regularization techniques to handle the increased complexity and prevent overfitting. These advancements have been crucial in achieving the high performance seen in modern CNNs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34cacd0d"
      },
      "source": [
        "## Question 6: Using keras, build and train a simple CNN model on the MNIST dataset from scratch. Include code for module creation, compilation, training, and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Load and preprocess the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape data for CNN\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32') / 255\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32') / 255\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Build the CNN model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=128,\n",
        "          epochs=10,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2kOnOk19wj7",
        "outputId": "ad4b1648-e4d7-45db-847c-39600c6d4abf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - accuracy: 0.8571 - loss: 0.4956 - val_accuracy: 0.9811 - val_loss: 0.0626\n",
            "Epoch 2/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9807 - loss: 0.0626 - val_accuracy: 0.9859 - val_loss: 0.0455\n",
            "Epoch 3/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9871 - loss: 0.0412 - val_accuracy: 0.9884 - val_loss: 0.0360\n",
            "Epoch 4/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9902 - loss: 0.0305 - val_accuracy: 0.9890 - val_loss: 0.0327\n",
            "Epoch 5/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9921 - loss: 0.0260 - val_accuracy: 0.9893 - val_loss: 0.0334\n",
            "Epoch 6/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9943 - loss: 0.0184 - val_accuracy: 0.9914 - val_loss: 0.0273\n",
            "Epoch 7/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9952 - loss: 0.0158 - val_accuracy: 0.9907 - val_loss: 0.0262\n",
            "Epoch 8/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9969 - loss: 0.0098 - val_accuracy: 0.9898 - val_loss: 0.0331\n",
            "Epoch 9/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9967 - loss: 0.0102 - val_accuracy: 0.9907 - val_loss: 0.0260\n",
            "Epoch 10/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9963 - loss: 0.0107 - val_accuracy: 0.9908 - val_loss: 0.0283\n",
            "Test loss: 0.02830616757273674\n",
            "Test accuracy: 0.9908000230789185\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a84f0209"
      },
      "source": [
        "## Question 7: Load and preprocess the CIFAR-10 dataset using Keras, and create a CNN model to classify RGB images. Show your preprocessing and architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e81dbd20",
        "outputId": "b97a88ef-64b6-4225-f8f7-cbca9b80f8b6"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Preprocessing\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "print(\"CIFAR-10 dataset loaded and preprocessed.\")\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"x_test shape:\", x_test.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n",
            "CIFAR-10 dataset loaded and preprocessed.\n",
            "x_train shape: (50000, 32, 32, 3)\n",
            "y_train shape: (50000, 10)\n",
            "x_test shape: (10000, 32, 32, 3)\n",
            "y_test shape: (10000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "id": "6340e590",
        "outputId": "a6ba8a82-5b59-49a4-b645-533a05fda036"
      },
      "source": [
        "# Build the CNN model\n",
        "model_cifar10 = Sequential()\n",
        "\n",
        "# Convolutional Layer 1\n",
        "model_cifar10.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)))\n",
        "model_cifar10.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
        "model_cifar10.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model_cifar10.add(Dropout(0.25))\n",
        "\n",
        "# Convolutional Layer 2\n",
        "model_cifar10.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model_cifar10.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model_cifar10.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model_cifar10.add(Dropout(0.25))\n",
        "\n",
        "# Flattening\n",
        "model_cifar10.add(Flatten())\n",
        "\n",
        "# Dense layers\n",
        "model_cifar10.add(Dense(512, activation='relu'))\n",
        "model_cifar10.add(Dropout(0.5))\n",
        "model_cifar10.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model_cifar10.compile(loss='categorical_crossentropy',\n",
        "                      optimizer='adam',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "# Show model architecture\n",
        "model_cifar10.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m9,248\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m36,928\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m2,097,664\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m5,130\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,097,664</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,130</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,168,362\u001b[0m (8.27 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,168,362</span> (8.27 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,168,362\u001b[0m (8.27 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,168,362</span> (8.27 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46a513e6",
        "outputId": "11c55179-85d5-4067-c1d6-a2aef5106622"
      },
      "source": [
        "# Train the model\n",
        "history = model_cifar10.fit(x_train, y_train,\n",
        "                            batch_size=64,\n",
        "                            epochs=20,\n",
        "                            verbose=1,\n",
        "                            validation_data=(x_test, y_test),\n",
        "                            shuffle=True)\n",
        "\n",
        "# Evaluate the model\n",
        "score = model_cifar10.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.8903 - loss: 0.3140 - val_accuracy: 0.7963 - val_loss: 0.6617\n",
            "Epoch 2/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.8908 - loss: 0.3046 - val_accuracy: 0.8015 - val_loss: 0.6581\n",
            "Epoch 3/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.8941 - loss: 0.2993 - val_accuracy: 0.7974 - val_loss: 0.6792\n",
            "Epoch 4/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.8961 - loss: 0.2963 - val_accuracy: 0.7984 - val_loss: 0.6812\n",
            "Epoch 5/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.8998 - loss: 0.2826 - val_accuracy: 0.7932 - val_loss: 0.6936\n",
            "Epoch 6/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.9012 - loss: 0.2726 - val_accuracy: 0.7992 - val_loss: 0.6888\n",
            "Epoch 7/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.9063 - loss: 0.2695 - val_accuracy: 0.7960 - val_loss: 0.6639\n",
            "Epoch 8/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.9057 - loss: 0.2676 - val_accuracy: 0.7980 - val_loss: 0.6935\n",
            "Epoch 9/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.9083 - loss: 0.2656 - val_accuracy: 0.7949 - val_loss: 0.7004\n",
            "Epoch 10/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9109 - loss: 0.2496 - val_accuracy: 0.7946 - val_loss: 0.7051\n",
            "Epoch 11/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.9139 - loss: 0.2506 - val_accuracy: 0.7935 - val_loss: 0.7245\n",
            "Epoch 12/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.9102 - loss: 0.2554 - val_accuracy: 0.7978 - val_loss: 0.7197\n",
            "Epoch 13/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9139 - loss: 0.2510 - val_accuracy: 0.7963 - val_loss: 0.7383\n",
            "Epoch 14/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9139 - loss: 0.2526 - val_accuracy: 0.7945 - val_loss: 0.7117\n",
            "Epoch 15/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.9161 - loss: 0.2426 - val_accuracy: 0.7979 - val_loss: 0.7276\n",
            "Epoch 16/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9189 - loss: 0.2323 - val_accuracy: 0.7962 - val_loss: 0.7071\n",
            "Epoch 17/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.9253 - loss: 0.2142 - val_accuracy: 0.7998 - val_loss: 0.7306\n",
            "Epoch 18/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9195 - loss: 0.2305 - val_accuracy: 0.7940 - val_loss: 0.7512\n",
            "Epoch 19/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.9216 - loss: 0.2325 - val_accuracy: 0.8026 - val_loss: 0.7084\n",
            "Epoch 20/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9226 - loss: 0.2224 - val_accuracy: 0.8006 - val_loss: 0.7335\n",
            "Test loss: 0.7334528565406799\n",
            "Test accuracy: 0.800599992275238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71c05f4c"
      },
      "source": [
        "## Question 8: Using PyTorch, write a script to define and train a CNN on the MNIST dataset. Include model definition, data loaders, training loop, and accuracy evaluation."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define the CNN model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        # Calculate the size of the flattened layer\n",
        "        # Input size is 28x28. After conv1 (3x3, padding=1) it's 28x28.\n",
        "        # After conv2 (3x3, padding=1) it's 28x28.\n",
        "        # After pool (2x2, stride=2) it's 14x14.\n",
        "        self.fc1 = nn.Linear(64 * 14 * 14, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = self.flatten(x)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Load and preprocess the MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "print(\"MNIST dataset loaded and preprocessed for PyTorch.\")\n",
        "\n",
        "# Define loss function and optimizer\n",
        "model_pytorch = Net()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_pytorch.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "epochs = 10\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_pytorch.to(device)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model_pytorch.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_pytorch(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "print(\"Finished Training\")\n",
        "\n",
        "# Evaluate the model\n",
        "model_pytorch.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model_pytorch(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Accuracy of the PyTorch model on the 10000 test images: {accuracy:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpjHzflrDhKx",
        "outputId": "ec8a8159-0370-43d3-fe47-14f78e38fe44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNIST dataset loaded and preprocessed for PyTorch.\n",
            "Epoch 1/10, Loss: 0.1124\n",
            "Epoch 2/10, Loss: 0.0353\n",
            "Epoch 3/10, Loss: 0.0214\n",
            "Epoch 4/10, Loss: 0.0154\n",
            "Epoch 5/10, Loss: 0.0092\n",
            "Epoch 6/10, Loss: 0.0110\n",
            "Epoch 7/10, Loss: 0.0066\n",
            "Epoch 8/10, Loss: 0.0060\n",
            "Epoch 9/10, Loss: 0.0048\n",
            "Epoch 10/10, Loss: 0.0062\n",
            "Finished Training\n",
            "Accuracy of the PyTorch model on the 10000 test images: 99.22%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1c96457"
      },
      "source": [
        "## Question 9: Given a custom image dataset stored in a local directory, write code using Keras ImageDataGenerator to preprocess and train a CNN model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c0d5d53",
        "outputId": "a1d9c7eb-11f5-466a-f790-6705b4f72923"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Create dummy directories for a simple dataset (e.g., two classes: 'class_a' and 'class_b')\n",
        "base_dir = './dummy_dataset'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "train_class_a_dir = os.path.join(train_dir, 'class_a')\n",
        "train_class_b_dir = os.path.join(train_dir, 'class_b')\n",
        "validation_class_a_dir = os.path.join(validation_dir, 'class_a')\n",
        "validation_class_b_dir = os.path.join(validation_dir, 'class_b')\n",
        "\n",
        "# Create the directories if they don't exist\n",
        "os.makedirs(train_class_a_dir, exist_ok=True)\n",
        "os.makedirs(train_class_b_dir, exist_ok=True)\n",
        "os.makedirs(validation_class_a_dir, exist_ok=True)\n",
        "os.makedirs(validation_class_b_dir, exist_ok=True)\n",
        "\n",
        "# Create some dummy images (simple colored squares)\n",
        "def create_dummy_image(directory, filename, color, size=(50, 50)):\n",
        "    img = Image.new('RGB', size, color=color)\n",
        "    img.save(os.path.join(directory, filename))\n",
        "\n",
        "# Create dummy images for training\n",
        "for i in range(10):\n",
        "    create_dummy_image(train_class_a_dir, f'image_a_{i}.png', color='red')\n",
        "    create_dummy_image(train_class_b_dir, f'image_b_{i}.png', color='blue')\n",
        "\n",
        "# Create dummy images for validation\n",
        "for i in range(5):\n",
        "    create_dummy_image(validation_class_a_dir, f'image_a_{i}.png', color='red')\n",
        "    create_dummy_image(validation_class_b_dir, f'image_b_{i}.png', color='blue')\n",
        "\n",
        "print(\"Dummy dataset created.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dummy dataset created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "221ba030",
        "outputId": "660bc5a5-05bd-4c95-fb4c-f1553c167aca"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# Define the path to the dummy dataset\n",
        "base_dir = './dummy_dataset'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "# Set up data generators for training and validation\n",
        "train_datagen = ImageDataGenerator(rescale=1./255) # Only rescaling for this simple example\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Flow training images in batches from the directory\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,                 # Path to the training directory\n",
        "    target_size=(50, 50),      # Resize images to match dummy image size\n",
        "    batch_size=4,\n",
        "    class_mode='categorical')  # Use categorical labels for classification\n",
        "\n",
        "# Flow validation images in batches from the directory\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,            # Path to the validation directory\n",
        "    target_size=(50, 50),\n",
        "    batch_size=4,\n",
        "    class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20 images belonging to 2 classes.\n",
            "Found 10 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "c9d22599",
        "outputId": "02de1588-cf50-4fe0-ad4c-87a2d3271601"
      },
      "source": [
        "# Build a simple CNN model\n",
        "model_custom_data = Sequential()\n",
        "\n",
        "model_custom_data.add(Conv2D(16, (3, 3), activation='relu', input_shape=(50, 50, 3)))\n",
        "model_custom_data.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model_custom_data.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "model_custom_data.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model_custom_data.add(Flatten())\n",
        "model_custom_data.add(Dense(64, activation='relu'))\n",
        "model_custom_data.add(Dense(train_generator.num_classes, activation='softmax')) # Output layer with number of classes\n",
        "\n",
        "# Compile the model\n",
        "model_custom_data.compile(loss='categorical_crossentropy',\n",
        "                          optimizer='adam',\n",
        "                          metrics=['accuracy'])\n",
        "\n",
        "# Show model architecture\n",
        "model_custom_data.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_9 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │           \u001b[38;5;34m448\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_7 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_10 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m4,640\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_8 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_3 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3872\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │       \u001b[38;5;34m247,872\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m130\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3872</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">247,872</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m253,090\u001b[0m (988.63 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">253,090</span> (988.63 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m253,090\u001b[0m (988.63 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">253,090</span> (988.63 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3ecf202",
        "outputId": "aa09027a-59dd-47c5-c627-1b25012f3aa1"
      },
      "source": [
        "# Train the model using the data generators\n",
        "epochs = 5 # Train for a few epochs on the dummy data\n",
        "\n",
        "history_custom_data = model_custom_data.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // validation_generator.batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 158ms/step - accuracy: 0.7931 - loss: 0.4465 - val_accuracy: 1.0000 - val_loss: 0.0417\n",
            "Epoch 2/5\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.0255 - val_accuracy: 1.0000 - val_loss: 0.0020\n",
            "Epoch 3/5\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 7.6240e-04 - val_accuracy: 1.0000 - val_loss: 4.7324e-05\n",
            "Epoch 4/5\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 1.9851e-05 - val_accuracy: 1.0000 - val_loss: 3.0398e-06\n",
            "Epoch 5/5\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 2.6766e-06 - val_accuracy: 1.0000 - val_loss: 4.1723e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e92b7391"
      },
      "source": [
        "Question 10: You are working on a web application for a medical imaging startup. Your\n",
        "task is to build and deploy a CNN model that classifies chest X-ray images into “Normal”\n",
        "and “Pneumonia” categories. Describe your end-to-end approach–from data preparation\n",
        "and model training to deploying the model as a web app using Streamlit.\n",
        "\n",
        "Here's an end-to-end approach for building and deploying a CNN model to classify chest X-ray images as \"Normal\" or \"Pneumonia,\" and serving it as a web application using Streamlit:\n",
        "\n",
        "Answer :\n",
        "\n",
        "**1. Data Preparation:**\n",
        "\n",
        "*   **Dataset Acquisition:** Obtain a suitable dataset of chest X-ray images with labels for \"Normal\" and \"Pneumonia.\" Publicly available datasets like the ChestX-ray8 dataset or the Kaggle Chest X-Ray Images (Pneumonia) dataset can be used. Ensure you have appropriate licenses for the data.\n",
        "*   **Data Loading and Exploration:** Load the images and their corresponding labels. Explore the dataset to understand the class distribution, image dimensions, and potential data imbalances.\n",
        "*   **Data Preprocessing:**\n",
        "    *   **Resizing:** Resize all images to a consistent size suitable for the CNN model (e.g., 224x224 pixels).\n",
        "    *   **Normalization:** Normalize pixel values to a standard range (e.g., 0-1) by dividing by 255.\n",
        "    *   **Data Augmentation:** Apply data augmentation techniques (e.g., rotation, zooming, flipping) to increase the size and diversity of the training data, which helps improve model generalization and reduce overfitting.\n",
        "*   **Splitting Data:** Split the dataset into training, validation, and testing sets. A common split is 70-15-15 or 80-10-10.\n",
        "\n",
        "**2. Model Building and Training:**\n",
        "\n",
        "*   **Choose a CNN Architecture:** Select an appropriate CNN architecture. You can build a model from scratch or use a pre-trained model (transfer learning) like VGG16, ResNet, or EfficientNet, fine-tuning it on your chest X-ray dataset. Transfer learning is often beneficial for medical imaging tasks with limited data.\n",
        "*   **Model Definition:** Define the CNN model using a framework like TensorFlow/Keras or PyTorch. The model will typically consist of convolutional layers, pooling layers, activation functions (e.g., ReLU), and fully connected layers for classification, with a final output layer having two units (for \"Normal\" and \"Pneumonia\") and a softmax activation.\n",
        "*   **Compilation:** Compile the model with an appropriate loss function (e.g., binary cross-entropy for two classes), an optimizer (e.g., Adam), and evaluation metrics (e.g., accuracy, precision, recall, F1-score).\n",
        "*   **Training:** Train the model on the training data, using the validation data to monitor performance and prevent overfitting. Use techniques like early stopping and learning rate scheduling if needed.\n",
        "\n",
        "**3. Model Evaluation:**\n",
        "\n",
        "*   **Evaluate on Test Set:** Evaluate the trained model on the held-out test set to assess its performance on unseen data. Calculate relevant metrics to understand the model's strengths and weaknesses.\n",
        "*   **Confusion Matrix:** Generate a confusion matrix to visualize the model's predictions and identify misclassifications.\n",
        "\n",
        "**4. Model Deployment (Streamlit Web App):**\n",
        "\n",
        "*   **Save the Trained Model:** Save the trained CNN model in a format suitable for deployment (e.g., Keras .h5, TensorFlow SavedModel, PyTorch .pth).\n",
        "*   **Build the Streamlit App:**\n",
        "    *   **Install Streamlit:** Install the Streamlit library (`pip install streamlit`).\n",
        "    *   **Create a Python Script:** Write a Python script using Streamlit to create the web application.\n",
        "    *   **Load the Model:** Load the saved trained model within the Streamlit script.\n",
        "    *   **File Uploader:** Add a file uploader component to allow users to upload chest X-ray images.\n",
        "    *   **Image Preprocessing:** Implement the same preprocessing steps used during training for the uploaded image.\n",
        "    *   **Prediction:** Use the loaded model to predict the class (Normal or Pneumonia) of the preprocessed image.\n",
        "    *   **Display Results:** Display the uploaded image and the model's prediction to the user.\n",
        "*   **Deployment:**\n",
        "    *   **Local Deployment:** Run the Streamlit app locally using `streamlit run your_app_script.py`.\n",
        "    *   **Cloud Deployment:** Deploy the Streamlit app to a cloud platform (e.g., Heroku, Google Cloud Platform, AWS, Streamlit Cloud). This typically involves creating a requirements file listing dependencies and configuring the deployment environment.\n",
        "\n",
        "**End-to-end Flow in the Web App:**\n",
        "\n",
        "1.  User uploads a chest X-ray image through the Streamlit interface.\n",
        "2.  The Streamlit app receives the image and preprocesses it.\n",
        "3.  The preprocessed image is fed into the loaded CNN model.\n",
        "4.  The model predicts whether the image shows a \"Normal\" chest or \"Pneumonia.\"\n",
        "5.  The Streamlit app displays the original image and the prediction result to the user.\n",
        "\n",
        "This end-to-end approach covers the key steps from data handling and model development to creating an interactive web application for practical use."
      ]
    }
  ]
}